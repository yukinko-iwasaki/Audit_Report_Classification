{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruption Prediction Based on the Audit Documents \n",
    "\n",
    "### Backgroud\n",
    "This project is motivated by the paper 'The Political Resource Curse' which studies the effect of additional government spending on the political corruptions in Brazil. The authors of the paper labeled each munincipality as corrupted or clean by reading audit documents. The purpose of this project is to predict the corruption of unlabeled munincipality based on the remaining audit documents, by using the already labeled audit documents as training dataset. Note that the audit documents are the products of 'Brazilian anti-corruption program' and only cover the randomly selected munincipalities.\n",
    "\n",
    "### Data\n",
    "- txt.zip - zip file containing the 2002 text files of audit documents\n",
    "- corruption_data.csv file containing the data on which munincipalities are corrupted\n",
    "- report_id_and_mun_code.csv text data which matches the report_id(audit document) and munincipality\n",
    "\n",
    "### Preprocessing\n",
    "First of all, we will preprocess all the audit documents so that they will be ready for visualization, feature extraction and classification.We will do the following for the preprocessing:\n",
    "- **lowering** the letters\n",
    "- removing **numbers**\n",
    "- removing **non-alphanumeric characters**\n",
    "- removing **underscores**\n",
    "- removing **stop words** (We will use NLTK to create a list of stop words)\n",
    "\n",
    "Then, we will lemmatize the documents by using spacy. Note that since each document is too large to be lemmatized, we will split each document into 5 sub documents to be processed.  \n",
    "\n",
    "\n",
    "First we will build helper functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pt_core_news_sm\n",
    "import math\n",
    "\n",
    "# getting a list of stopwords\n",
    "stop = stopwords.words('portuguese')\n",
    "#getting the lemmatizer\n",
    "nlp = pt_core_news_sm.load()\n",
    "\n",
    "## Helpher method for the preprocess to join a text\n",
    "def join(text):\n",
    "    return ' '.join(word for word in text)\n",
    "\n",
    "## Method for preprocessing the document\n",
    "def preprocess(text):\n",
    "    #lowercase the text\n",
    "    text = text.lower()\n",
    "    #remove number\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #remove non-alphabetical letter\n",
    "    text = re.sub(r'[^\\w\\s]', ' ',text )\n",
    "    #get rid of the underscore\n",
    "    text = text.replace('_',\" \")\n",
    "    #get rid of a special letter in portuguese\n",
    "    text = text.replace('nº',\" \")\n",
    "    #removes stopword and a single letter\n",
    "    l = [w for w in text.split() if w not in stop and len(w)>1]\n",
    "    return join(l)\n",
    "\n",
    "## Method for lemmatizing a document\n",
    "def lemmatizer(text):\n",
    "    text = text.split()\n",
    "    text1 = join(text[0:math.floor(len(text)/5)])\n",
    "    text2 = join(text[math.floor(len(text)/5):math.floor(2*len(text)/5)])\n",
    "    text3 = join(text[math.floor(2*len(text)/5):math.floor(3*len(text)/5)])\n",
    "    text4 = join(text[math.floor(3*len(text)/5):math.floor(4*len(text)/5)])\n",
    "    text5 = join(text[math.floor(4*len(text)/5):])\n",
    "\n",
    "    text1 = nlp(text1)\n",
    "    l1 = [word.lemma_ for word in text1]\n",
    "    text2 = nlp(text2)\n",
    "    l2 = [word.lemma_ for word in text2]\n",
    "    text3 = nlp(text3)\n",
    "    l3 = [word.lemma_ for word in text3]\n",
    "    text4 = nlp(text4)\n",
    "    l4 = [word.lemma_ for word in text4]\n",
    "    text5 = nlp(text5)\n",
    "    l5 = [word.lemma_ for word in text5]\n",
    "    return join(l1) + \" \" + join(l2) + \" \" + join(l3) + \" \" + join(l4) + \" \" + join(l5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the audit documents as text and merge them with target label(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "## reading the report data \n",
    "path = \"txt.zip\"\n",
    "zip_ref = zipfile.ZipFile(path,'r')\n",
    "zip_ref.extractall('data/reports')\n",
    "df = pd.DataFrame()\n",
    "num = 1\n",
    "basepath = 'data/reports'\n",
    "while True:\n",
    "    path = os.path.join(basepath,str(num)+'.txt')\n",
    "    try:\n",
    "        f = open(path,\"r\",encoding='utf-8')\n",
    "        contents = f.read()\n",
    "        df = df.append([[contents,num]],ignore_index = True)\n",
    "        num = num+1\n",
    "    except:\n",
    "        break\n",
    "df.columns = ['contents','report_id']\n",
    "\n",
    "## reading the reference data(municipality_id to report_id) for the later merge\n",
    "basepath = 'data/corruption'\n",
    "path = os.path.join(basepath,'report_id_and_mun_code.csv')\n",
    "df2 = pd.read_csv(path)\n",
    "\n",
    "##reading the corruption data(label)\n",
    "path = os.path.join(basepath,'corruption_data.csv')\n",
    "df3 = pd.read_csv(path,encoding = 'utf-8');\n",
    "\n",
    "## merge the report data with reference data, attach corresponding munincipality_id\n",
    "df = df.merge(df2, on = 'report_id');\n",
    "\n",
    "##create dataset consisting of 2 classes 'narrow' and 'broad' corruption with munincipality_id\n",
    "broad = pd.DataFrame(df3.groupby(['id_city'], sort=False)['broad'].max())\n",
    "narrow = pd.DataFrame(df3.groupby(['id_city'], sort=False)['narrow'].max())\n",
    "df4 = broad.merge(narrow,on = 'id_city')\n",
    "\n",
    "## merge the label class data to the text data\n",
    "df = df.merge(df4,left_on= 'mun_code', right_on = 'id_city')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preprocess and lemmatize the audit document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['contents','mun_code','broad','narrow', 'report_id']]\n",
    "\n",
    "## preprocess the text data\n",
    "df['contents'] = df['contents'].apply(preprocess)\n",
    "## lemmatize the text data\n",
    "df['contents'] = df['contents'].apply(lemmatizer)\n",
    "\n",
    "## saves the preprocessed data as csv\n",
    "df.to_csv('data/corruption/processed.csv',index = False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process information about the document such as number of pages and images per document from the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the report data \n",
    "import fitz\n",
    "\n",
    "df_feature = pd.DataFrame();\n",
    "num = 1\n",
    "\n",
    "basepath = 'data/report_pdf'\n",
    "while True:\n",
    "    path = os.path.join(basepath,str(num)+'.pdf')\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        num_img = 0\n",
    "        page = len(doc)\n",
    "        for i in range(len(doc)):\n",
    "            num_img += len(doc.getPageImageList(i))\n",
    "        df_feature = df_feature.append([[num_img,page,num]],ignore_index = True)\n",
    "        num = num+1\n",
    "    except:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.columns = ['num_img','page','report_id']\n",
    "\n",
    "## merges the feature data with the text data\n",
    "df = df.merge(df_feature, on = 'report_id');\n",
    "## saves the preprocessed data as csv\n",
    "df.to_csv('data/corruption/processed.csv',index = False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "Before jumping into the classification, let's look at the basic summary of the data\n",
    "- basic statistics\n",
    "- statistics on word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>mun_code</th>\n",
       "      <th>broad</th>\n",
       "      <th>narrow</th>\n",
       "      <th>report_id</th>\n",
       "      <th>num_img</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presidência repúblico controladoria geral uniã...</td>\n",
       "      <td>2201739</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1861</td>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>presidência repúblico controladoria geral uniã...</td>\n",
       "      <td>3165404</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>823</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>presidência repúblico controladoria geral uniã...</td>\n",
       "      <td>4213401</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>492</td>\n",
       "      <td>22</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidência repúblico controladoria geral uniã...</td>\n",
       "      <td>2504900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>presidência repúblico controladoria geral uniã...</td>\n",
       "      <td>4104907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents  mun_code  broad  narrow  \\\n",
       "0  presidência repúblico controladoria geral uniã...   2201739      1       0   \n",
       "1  presidência repúblico controladoria geral uniã...   3165404      0       0   \n",
       "2  presidência repúblico controladoria geral uniã...   4213401      1       1   \n",
       "3  presidência repúblico controladoria geral uniã...   2504900      1       0   \n",
       "4  presidência repúblico controladoria geral uniã...   4104907      0       0   \n",
       "\n",
       "   report_id  num_img  page  \n",
       "0       1861       10   115  \n",
       "1        823       11    38  \n",
       "2        492       22    68  \n",
       "3        472       17    65  \n",
       "4        419       15    47  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable \"contents\" is the actual content of the document, and the narrow variable is the target variable indicating if the munincipality is corrupt or not(1: corrupt, 0: otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of total documents:  1682\n",
      "Total number of documents for corrupted muncipalities:  943\n",
      "Total number of documents for clean muncipalities:  739\n"
     ]
    }
   ],
   "source": [
    "## counts the number of documents for corrupted and clean munincipalities\n",
    "nclean = len(df[df['narrow']==0])\n",
    "nnarrow = len(df[df['narrow']==1])\n",
    "print('Total number of total documents: ',len(df))\n",
    "print('Total number of documents for corrupted muncipalities: ', nnarrow)\n",
    "print('Total number of documents for clean muncipalities: ', nclean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have relatively balanced dataset. Let's now look at how diverse the words are across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numner of words for corrupted : 24184\n",
      "Numner of words for clean : 28133\n",
      "Number of words in total 29202\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import operator\n",
    "\n",
    "## reads the preprocessed data\n",
    "df = pd.read_csv('data/corruption/processed.csv',encoding = 'utf-8')\n",
    "\n",
    "## creating the tokenizer objects\n",
    "c = Tokenizer()\n",
    "n = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "c.fit_on_texts(df[df['narrow'] == 0]['contents'])\n",
    "n.fit_on_texts(df[df['narrow'] == 1]['contents'])\n",
    "\n",
    "cor= set(c.word_counts.keys())\n",
    "nar = set(n.word_counts.keys())\n",
    "print('Numner of words for corrupted :', len(cor))\n",
    "print('Numner of words for clean :', len(nar))\n",
    "print('Number of words in total', len(cor.union(nar)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be much differences in the variety of words used in both documents. There also seems to be lots of overlapping words.\n",
    "\n",
    "\n",
    "We will now see how long each documents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length is  1179368\n",
      "The minimum length is  3173\n",
      "The median length of documents for corrupt munincipalitie is:  126986\n",
      "The median length of documents for clean munincipalitie is:  83848\n"
     ]
    }
   ],
   "source": [
    "import statistics as stat\n",
    "df['length'] = df['contents'].apply(len)\n",
    "\n",
    "print('The maximum length is ', max(df['length']))\n",
    "print('The minimum length is ', min(df['length']))\n",
    "\n",
    "print('The median length of documents for corrupt munincipalitie is: ',stat.median(df[df['narrow'] == 1]['length']))\n",
    "print('The median length of documents for clean munincipalitie is: ',stat.median(df[df['narrow'] == 0]['length']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11b840c0ba8>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEICAYAAACAgflvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X2UXVWZ5/Hvzwp5IUACldgNCViFxB4TxCaUgIINi9gQGiS4JhkKkZcBBxHQaRhWE8ZpO9DJLKNoBAGBblBAsIgZu0krPZnWoC4UCRXDixWIFAl2itCYd42akCLP/HF24s31vqVy69yq5PdZ666cs8/ezz77nKSenHN3naOIwMzMLE9va/QOmJnZ/sfJx8zMcufkY2ZmuXPyMTOz3Dn5mJlZ7px8zMwsd04+NuBI6pJ0eqP3o5EkfUTSaklbJB3f4H1pkRSShjSg78skPZl3v9b/nHwsV5JelfShorLdfsBExKSI+EGVOA37gZiTW4FrI+KgiFhWvDGN/Zh6dCTpB5I+Xo9Ye7kf+/o5tQJOPmYlDIAfgO8Auhq8D2b9xsnHBpzCqyNJJ0rqlPRrSW9I+lKq9qP056Z0a+r9kt4m6X9J+qWkX0l6UNKogriXpG3rJf1tUT+zJC2Q9A1JvwYuS30/JWmTpNcl3SFpaEG8kHS1pJcl/UbS30t6Z2rza0nzC+sXjbHkvkoaJmkL0AQ8J+mVEm13jv25NPYLUvm5kp5N+/sTScel8ndK2iBpclo/QtI6SadLmgN8ELgjxbqjhvMzStJ96Zi8Jmm2pKa07TJJT0q6VdJGSasknV3QtlXSj9Lx+p6kOyV9o9w5LWhXMp4NYhHhjz+5fYBXgQ8VlV0GPFmqDvAUcHFaPgg4OS23AAEMKWh3OdANHJ3qfht4KG2bCGwBTgWGkt3W2l7Qz6y0fj7Zf8pGACcAJwNDUn8vAn9d0F8AC4FDgEnANuD7qf9RwHLg0jLHoey+FsQ+psJx3G07MBn4FXASWeK6NB3HYWn7f0v7fyCwCLi1oO0PgI9X6Gu3Yw38M3APMBJ4O7AE+ETBudye+msCPgmsAVRwPm9N5+BU4NfANyqc04rx/Bm8n4bvgD/71yf9QNwCbCr4/I7yyedHwM3AmKI4pX5QfR+4umD9z9IPriHAZ4FvFmw7EHiT3ZPPj6rs+18D/1SwHsApBetLgRsL1r8IfLlMrLL7WhB7T5LPV4G/L6qzAjitYH0h8ALw/M6klMprTj7An5Al2REF2y8EnkjLlwHdRcc5gD8FjgJ6gQMLtn+jhuRTMl6j/y77s3cf33azRjg/Ikbv/ABXV6h7BfAu4CVJz0g6t0LdI4BfFqz/kj/8wDwCWL1zQ0T8Dlhf1H514Yqkd0n6jqT/SLfi/jcwpqjNGwXLvy+xflAf9rUv3gH8j3TLbZOkTcCRqZ+d/gE4FvhKRGzbi34OAF4v6Ocesiugnf5j50I6zpAdhyOADQVlUHTMyygXzwYxJx8b0CLi5Yi4kOyH21xggaSRZP/7LbaG7IfjTjv/p/0G8DowfucGSSOA5uLuita/CrwETIiIQ4D/Cajvo6l5X/tiNTCnMKlHxIER8U0ASQcBXwbuA2ZJOqyg7Z482n412ZXPmIJ+DomISTW0fR04TNKBBWVH9nE/bJBz8rEBTdLHJI2NiB1kt+gA3gLWAjvIvjPZ6ZvAdelL7YPIrlQejYheYAHwYUkfSJMAbqZ6IjmY7DuJLZL+E9n3DfVSaV9r8Qa7j/0fgKsknaTMSEnnSDo4bb8NWBoRHwe+C9xdIVZZEfE68P+AL0o6JE2ceKek02po+0ugkyz5DU0TCj5cUKXUObV9lJOPDXRTga40A+w2oD0itqbbL3OAH6fbPycD9wMPkX1PtArYCnwKICK60nIH2f/Af0P2BX2l2083AB9Ndf8BeLSO4yq7rzWaBTyQxv5fIqKT7Ev5O4CNZJMZLgOQNI3sOF6V2l4PTJZ0UVq/DZieZpPdXkPfl5BNGFie+loAHF7jfl8EvJ/sludssmO6DXbdUis+p7aP2jkDxWy/kq42NpHdUlvV6P3ZX0l6FHgpIv6u0fti+fKVj+03JH1Y0oHpO6NbyWZ+vdrYvdq/SHpfuk33NklTgWlkU7dtP+PkY/uTaWRf9K8BJpDdwvOlf77+lGxq9xbgduCTUeLxQbbv8203MzPLna98zMwsd41+eGJDjBkzJlpaWhq9G2Zmg8rSpUvXRcTYesTaL5NPS0sLnZ2djd4NM7NBRdIvq9eqjW+7mZlZ7px8zMwsd04+ZmaWu/3yOx8zs+3bt9PT08PWrVsbvSsDzvDhwxk/fjwHHHBAv/Xh5GNm+6Wenh4OPvhgWlpakOr1sPLBLyJYv349PT09tLa29ls/vu1mZvulrVu30tzc7MRTRBLNzc39fkXo5GNm+y0nntLyOC5OPmZmljt/52NmBpzyucW8tun3dYs3bvQIfjzzjLrFq7evf/3rnHnmmRxxxBHVK/cDJ59Gm/ce2Pzv9Yk16ii47oX6xDLbz7y26fe8+rlz6havZeZ36xarUG9vL0OGDCm7Xquvf/3rHHvssQM7+aT3btwGNAH/GBGfK9o+DHgQOIHsDYUXRMSradtNwBVkrz7+dEQsqhRTUivZ2yYPA34GXBwRb1bp4zjgHuAQstfwvi8iBsf8yc3/DrM21yfWrFH1iWNmuXjwwQe59dZbkcRxxx3H7Nmzufzyy1m7di1jx47la1/7GkcddRSXXXYZhx12GMuWLWPy5MkcfPDBrFmzhldffZUxY8Zw5pln0tnZyR133AHAueeeyw033MDpp5/OQQcdxCc+8QmeeOIJDj30UDo6OvjhD39IZ2cnF110ESNGjOCpp55ixIgRuY696nc+kpqAO4GzgYnAhZImFlW7AtgYEccA84C5qe1EoB2YRPYa37skNVWJOReYFxETyF7Re0WVPoYA3wCuiohJwOnA9j08DmZmuerq6mLOnDksXryY5557jttuu41rr72WSy65hOeff56LLrqIT3/607vq/+IXv+B73/seX/ziFwFYunQpjz32GI888kjFfn77298yefJkfvazn3Haaadx8803M336dNra2nj44Yd59tlnc088UNuEgxOB7ohYGRFvkl2VTCuqMw14IC0vAKYomy4xDeiIiG3pVcXdKV7JmKnNGSkGKeb5Vfo4E3g+Ip4DiIj1EfFW7YfAzCx/ixcvZvr06YwZMwaAww47jKeeeoqPfvSjAFx88cU8+eSTu+rPmDGDpqamXevnnXdeTUnjbW97GxdccAEAH/vYx3aL2Ui1JJ9xwOqC9Z5UVrJORPQCm4HmCm3LlTcDm1KM4r7K9fEuICQtkvQzSX9TahCSrpTUKalz7dq1NQzbzKz/RETVKc2F20eOHLnbtsL1IUOGsGPHjl3rlX5HZ6BML6/lO59Se1r8+tNydcqVl0p6lepX6mMIcCrwPuB3wPclLY2I7+9WMeJe4F6Atra2vXt9a70nCZjZfmfKlCl85CMf4brrrqO5uZkNGzbwgQ98gI6ODi6++GIefvhhTj311JpitbS0cNddd7Fjxw5ee+01lixZsmvbjh07WLBgAe3t7TzyyCO7Yh588MH85je/6Zex1aKW5NMDHFmwPh5YU6ZOT/oOZhSwoUrbUuXrgNGShqSrm8L6lfr4YUSsA5D0ODAZ2C351FU9JwmY2YAwbvSIus5QGze68i2xSZMm8ZnPfIbTTjuNpqYmjj/+eG6//XYuv/xyvvCFL+yacFCLU045hdbWVt7znvdw7LHHMnny5F3bRo4cSVdXFyeccAKjRo3i0UcfBeCyyy7jqquuatiEAyKi4ocsQa0EWoGhwHPApKI61wB3p+V2YH5anpTqD0vtV5LNbisbE/gW0J6W7waurtLHoWSz4g5Mcb8HnFNpTCeccELslb87ZO/a95eBul9mA9Dy5csbvQu5GDlyZJ/alTo+QGdUyRm1fqpe+UREr6RrgUUpcdwfEV2Sbkk7shC4D3hIUjfZ1Uh7atslaT6wHOgFrok0GaBUzNTljUCHpNnAshSbCn1slPQl4Bmy23CPR0T/TLA3M7O6qOn3fCLiceDxorLPFixvBWaUaTsHmFNLzFS+kmw2XHF5pT6+QTbd2szMCmzZsqXRu1CSn+1mZma5c/IxM7PcOfmYmVnunHzMzCx3fqq1mRnU95fHoU9PmZ81axYHHXQQN9xwQ/32Y4By8jEzg/r/8rifMl+Rb7uZmTXIgw8+yHHHHcd73/teLr744t22vfLKK0ydOpUTTjiBD37wg7z00ksA/Mu//AsnnXQSxx9/PB/60Id44403gOyq6fLLL+f000/n6KOP5vbbb899PHvCycfMrAFKvVKh0JVXXslXvvIVli5dyq233srVV18NwKmnnspPf/pTli1bRnt7O5///Od3tXnppZdYtGgRS5Ys4eabb2b79oH7dhnfdjMza4BSr1TYacuWLfzkJz9hxow//F79tm3bAOjp6eGCCy7g9ddf580336S1tXVXnXPOOYdhw4YxbNgw3v72t/PGG28wfvz4nEa0Z5x8zMwaICq8UmHHjh2MHj2aZ5999o+2fepTn+L666/nvPPO4wc/+AGzZs3atW3YsGG7lpuamujt7f2j9gOFb7uZmTXAlClTmD9/PuvXrwdgw4YNu7YdcsghtLa28q1vfQvIEtVzzz0HwObNmxk3LnvN2QMPPMBg5SsfMzPIpkbXc4ZalXd1lXqlQktLy67tDz/8MJ/85CeZPXs227dvp729nfe+973MmjWLGTNmMG7cOE4++WRWrVpVv33OkbKnZO9f2traorOzs+8BZo0amO/zGaj7ZTYAvfjii7z73e9u9G4MWKWOT3pRZ1s94vu2m5mZ5c7Jx8zMcufkY2b7rf3xa4da5HFcnHzMbL80fPhw1q9f7wRUJCJYv349w4cP79d+PNvNzPZL48ePp6enh7Vr1zZ6Vwac4cOH9/svpzr5mNl+6YADDtjt6QCWL992MzOz3Dn5mJlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlrqbkI2mqpBWSuiXNLLF9mKRH0/anJbUUbLspla+QdFa1mJJaU4yXU8yhlfqQ1CLp95KeTZ+7+3owzMwsH1WTj6Qm4E7gbGAicKGkiUXVrgA2RsQxwDxgbmo7EWgHJgFTgbskNVWJOReYFxETgI0pdtk+klci4s/T56o9OgJmZpa7Wq58TgS6I2JlRLwJdADTiupMA3a+1WgBMEXZK/qmAR0RsS0iVgHdKV7JmKnNGSkGKeb5VfowM7NBppbkMw5YXbDek8pK1omIXmAz0FyhbbnyZmBTilHcV7k+AFolLZP0Q0kfLDUISVdK6pTU6cdpmJk1Vi3Jp9TVRfGT+MrVqVd5pT5eB46KiOOB64FHJB3yRxUj7o2ItohoGzt2bIlQZmaWl1qSTw9wZMH6eGBNuTqShgCjgA0V2pYrXweMTjGK+yrZR7qltx4gIpYCrwDvqmFcZmbWILUkn2eACWkW2lCyCQQLi+osBC5Ny9OBxZE9p3wh0J5mqrUCE4Al5WKmNk+kGKSYj1XqQ9LYNIEBSUenPlbWfgjMzCxvVZ9qHRG9kq4FFgFNwP0R0SXpFqAzIhYC9wEPSeomu+JpT227JM0HlgO9wDUR8RZAqZipyxuBDkmzgWUpNuX6AP4CuEVSL/AWcFVEbOj7ITEzs/6m/fFFSm1tbdHZ2dn3ALNGwazN9duhehmo+2Vm+wRJSyOirR6x/IQDMzPLnZOPmZnlzsnHzMxy5+RjZma5c/IxM7PcOfmYmVnunHzMzCx3Tj5mZpY7Jx8zM8udk4+ZmeXOycfMzHLn5GNmZrlz8jEzs9w5+ZiZWe6cfMzMLHdOPmZmljsnHzMzy52Tj5mZ5c7Jx8zMcufkY2ZmuXPyMTOz3Dn5mJlZ7px8zMwsd04+ZmaWOycfMzPLXU3JR9JUSSskdUuaWWL7MEmPpu1PS2op2HZTKl8h6axqMSW1phgvp5hDq/WRth8laYukG/b0IJiZWb6qJh9JTcCdwNnAROBCSROLql0BbIyIY4B5wNzUdiLQDkwCpgJ3SWqqEnMuMC8iJgAbU+yyfRSYB/xrrQM3M7PGqeXK50SgOyJWRsSbQAcwrajONOCBtLwAmCJJqbwjIrZFxCqgO8UrGTO1OSPFIMU8v0ofSDofWAl01T50MzNrlFqSzzhgdcF6TyorWScieoHNQHOFtuXKm4FNKUZxXyX7kDQSuBG4udIgJF0pqVNS59q1a6sM2czM+lMtyUclyqLGOvUqr9THzWS36baU2P6HihH3RkRbRLSNHTu2UlUzM+tnQ2qo0wMcWbA+HlhTpk6PpCHAKGBDlbalytcBoyUNSVc3hfXL9XESMF3S54HRwA5JWyPijhrG1mctM7/bn+FLGjd6BD+eeUbu/ZqZ1VstyecZYIKkVuA1sgkEHy2qsxC4FHgKmA4sjoiQtBB4RNKXgCOACcASsquYP4qZ2jyRYnSkmI9V6gP44M6dkDQL2NLfiQfg1c+d099d/JFGJDwzs/5QNflERK+ka4FFQBNwf0R0SboF6IyIhcB9wEOSusmuRtpT2y5J84HlQC9wTUS8BVAqZuryRqBD0mxgWYpNuT7MzGzwqeXKh4h4HHi8qOyzBctbgRll2s4B5tQSM5WvJJsNV1xeto+COrMqbTczs4HBTzgwM7PcOfmYmVnunHzMzCx3Tj5mZpY7Jx8zM8udk4+ZmeXOycfMzHLn5GNmZrlz8jEzs9w5+ZiZWe6cfMzMLHdOPmZmljsnHzMzy52Tj5mZ5a6mVyrYwDBu9IiKL5R7dXj/vHDOb1A1s3pz8hlEqiaAWf3zhlW/QdXM6s233czMLHdOPmZmljsnHzMzy52Tj5mZ5c7Jx8zMcufkY2ZmuXPyMTOz3Dn5mJlZ7mpKPpKmSlohqVvSzBLbh0l6NG1/WlJLwbabUvkKSWdViympNcV4OcUcWqkPSSdKejZ9npP0kb4eDDMzy0fV5COpCbgTOBuYCFwoaWJRtSuAjRFxDDAPmJvaTgTagUnAVOAuSU1VYs4F5kXEBGBjil22D+DnQFtE/Hnq4x5JfnKDmdkAVsuVz4lAd0SsjIg3gQ5gWlGdacADaXkBMEWSUnlHRGyLiFVAd4pXMmZqc0aKQYp5fqU+IuJ3EdGbyocDUevgzcysMWpJPuOA1QXrPamsZJ2UCDYDzRXalitvBjYVJJPCvsr1gaSTJHUBLwBXFbQ3M7MBqJbkoxJlxVcX5erUq7zifkTE0xExCXgfcJOk4cUVJV0pqVNS59q1a0uEMjOzvNSSfHqAIwvWxwNrytVJ37eMAjZUaFuufB0wuuA7m8K+yvWxS0S8CPwWOLZ4EBFxb0S0RUTb2LFjqw7azMz6Ty3J5xlgQpqFNpRsAsHCojoLgUvT8nRgcUREKm9PM9VagQnAknIxU5snUgxSzMcq9ZFiDAGQ9A7gz4BXaz4CZmaWu6qzwiKiV9K1wCKgCbg/Irok3QJ0RsRC4D7gIUndZFcj7altl6T5wHKgF7gmIt4CKBUzdXkj0CFpNrAsxaZcH8CpwExJ24EdwNURsa7vh8TMzPpbTVOSI+Jx4PGiss8WLG8FZpRpOweYU0vMVL6SbDZccXnJPiLiIeChqoMwM7MBw084MDOz3Dn5mJlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlzsnHzMxy5+RjZma5c/IxM7PcOfmYmVnunHzMzCx3Tj5mZpY7Jx8zM8udk4+ZmeXOycfMzHLn5GNmZrlz8jEzs9w5+ZiZWe6cfMzMLHdOPmZmljsnHzMzy52Tj5mZ5c7Jx8zMcufkY2ZmuXPyMTOz3NWUfCRNlbRCUrekmSW2D5P0aNr+tKSWgm03pfIVks6qFlNSa4rxcoo5tFIfkv5S0lJJL6Q/z+jrwTAzs3xUTT6SmoA7gbOBicCFkiYWVbsC2BgRxwDzgLmp7USgHZgETAXuktRUJeZcYF5ETAA2pthl+wDWAR+OiPcAlwIP7dkhMDOzvNVy5XMi0B0RKyPiTaADmFZUZxrwQFpeAEyRpFTeERHbImIV0J3ilYyZ2pyRYpBinl+pj4hYFhFrUnkXMFzSsFoPgJmZ5a+W5DMOWF2w3pPKStaJiF5gM9BcoW258mZgU4pR3Fe5Pgr9Z2BZRGwrHoSkKyV1Supcu3ZtlSGbmVl/qiX5qERZ1FinXuVV90PSJLJbcZ8oUY+IuDci2iKibezYsaWqmJlZTmpJPj3AkQXr44E15epIGgKMAjZUaFuufB0wOsUo7qtcH0gaD/wTcElEvFLDmMzMrIFqST7PABPSLLShZBMIFhbVWUj2ZT/AdGBxREQqb08z1VqBCcCScjFTmydSDFLMxyr1IWk08F3gpoj48Z4M3szMGqNq8knfr1wLLAJeBOZHRJekWySdl6rdBzRL6gauB2amtl3AfGA58H+BayLirXIxU6wbgetTrOYUu2wfKc4xwN9KejZ93t7H42FmZjkYUr0KRMTjwONFZZ8tWN4KzCjTdg4wp5aYqXwl2Wy44vKSfUTEbGB21UGYmdmA4SccmJlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlzsnHzMxy5+RjZma5c/IxM7PcOfmYmVnunHzMzCx3Tj5mZpY7Jx8zM8udk4+ZmeXOycfMzHLn5GNmZrlz8jEzs9zV9DI5GyRGHQWzRtUv1nUv1CeWmVkRJ599ST2TRb2SmJlZCb7tZmZmuXPyMTOz3Dn5mJlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlrqbkI2mqpBWSuiXNLLF9mKRH0/anJbUUbLspla+QdFa1mJJaU4yXU8yhlfqQ1CzpCUlbJN3R1wNhZmb5qZp8JDUBdwJnAxOBCyVNLKp2BbAxIo4B5gFzU9uJQDswCZgK3CWpqUrMucC8iJgAbEyxy/YBbAX+FrhhD8duZmYNUsuVz4lAd0SsjIg3gQ5gWlGdacADaXkBMEWSUnlHRGyLiFVAd4pXMmZqc0aKQYp5fqU+IuK3EfEkWRIyM7NBoJbkMw5YXbDek8pK1omIXmAz0FyhbbnyZmBTilHcV7k+aiLpSkmdkjrXrl1bazMzM+sHtSQflSiLGuvUq7zW/SgrIu6NiLaIaBs7dmytzczMrB/Uknx6gCML1scDa8rVkTQEGAVsqNC2XPk6YHSKUdxXuT7MzGyQqSX5PANMSLPQhpJNIFhYVGchcGlang4sjohI5e1pplorMAFYUi5mavNEikGK+ViVPszMbJCp+kqFiOiVdC2wCGgC7o+ILkm3AJ0RsRC4D3hIUjfZ1Uh7atslaT6wHOgFromItwBKxUxd3gh0SJoNLEuxKddHivUqcAgwVNL5wJkRsbyvB8XMzPpXTe/ziYjHgceLyj5bsLwVmFGm7RxgTi0xU/lKstlwxeWV+mipOAAzMxtQ/IQDMzPLnd9kalWNGz2ClpnfbVjfP555RkP6NrP+4+RjVTXyh3+jkp6Z9S/fdjMzs9w5+ZiZWe6cfMzMLHdOPmZmljsnHzMzy52Tj5mZ5c7Jx8zMcufkY2ZmuXPyMTOz3Dn5mJlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlzsnHzMxy5+RjZma5c/IxM7Pc+U2mNqA16hXefn23Wf9y8rEBrVEJwK/vNutfvu1mZma585WPlTbqKJg1qj5xrnth7+OY2T7FycdKq1fCqEcCM7N9Tk233SRNlbRCUrekmSW2D5P0aNr+tKSWgm03pfIVks6qFlNSa4rxcoo5tK99mJnZwFT1ykdSE3An8JdAD/CMpIURsbyg2hXAxog4RlI7MBe4QNJEoB2YBBwBfE/Su1KbcjHnAvMiokPS3Sn2V/e0j4h4a28OjNVJvW7f7YyV0y28Rs2y29m3Z9rZvq6W224nAt0RsRJAUgcwDShMPtOAWWl5AXCHJKXyjojYBqyS1J3iUSqmpBeBM4CPpjoPpLhf7UMfT9V4DKw/1TNZzHtPbomskT/8T/nc4rokvieHfZrxWrfXcXpiDKduu32v4wxEjUr0p3xuMa9t+n3u/cLA+c9NLclnHLC6YL0HOKlcnYjolbQZaE7lPy1qOy4tl4rZDGyKiN4S9fvSxy6SrgSuTKtbJK0oP+SKxgDruFl9bD6gZGPZd+zBeH4O1w/4c7hX5+fIuu3Gr4Fz6xFowP19+yWgm/rUdMCNpVZlxlzreN5Rr/2oJfmU+hcaNdYpV17qu6ZK9fvSx+4FEfcC95aou0ckdUZE297GGQj2pbGAxzPQ7Uvj2ZfGAo0ZTy0TDnrY/T9R44E15epIGgKMAjZUaFuufB0wOsUo7mtP+zAzswGqluTzDDAhzUIbSvbl/sKiOguBS9PydGBxREQqb08z1VqBCcCScjFTmydSDFLMx/rYh5mZDVBVb7ul71euBRYBTcD9EdEl6RagMyIWAvcBD6Uv+zeQJRNSvflkkxN6gWt2zkIrFTN1eSPQIWk2sCzFpi999JO9vnU3gOxLYwGPZ6Dbl8azL40FGjAeZRcPZmZm+fGz3czMLHdOPmZmljsnnxpVe8RQzvtypKQnJL0oqUvSf0/lh0n6t/Roon+TdGgql6Tb074/L2lyQaxLU/2XJV1aUH6CpBdSm9vTL/SW7aNO42qStEzSd9J63R61tKePc6rDWEZLWiDppXSe3j+Yz4+k69LftZ9L+qak4YPp/Ei6X9KvJP28oKxh56NSH30cyxfS37XnJf2TpNEF2wbmI84iwp8qH7JJEa8ARwNDgeeAiQ3cn8OByWn5YOAXwETg88DMVD4TmJuW/wr4V7LfiToZeDqVHwasTH8empYPTduWAO9Pbf4VODuVl+yjTuO6HngE+E5anw+0p+W7gU+m5auBu9NyO/BoWp6Yzs0woDWds6ZK569cH3UYywPAx9PyUGD0YD0/ZL+0vQoYUXDMLhtM5wf4C2Ay8POCsoadj3J97MVYzgSGpOW5Bf3U7Zjv6XmtOo56/eDYlz/pL9WigvWbgJsavV8F+/MY2XPyVgCHp7LDgRVp+R7gwoL6K9L2C4F7CsrvSWWHAy8VlO+qV66POoxhPPB9sscrfSf9o1xX8A+BpHBVAAADHElEQVRq1zkgmyX5/rQ8JNVT8XnZWa/c+avUx16O5RCyH9YqKh+U54c/PF3ksHS8vwOcNdjOD9DC7j+wG3Y+yvXR17EUbfsI8HDhsazHMd/T81ptDL7tVptSjxj6o0f4NEK69D0eeBr4k4h4HSD9+fZUrdz+VyrvKVFOhT721peBvwF2pPWaH7UEFD5qaU/GWamPvXE0sBb4mrLbiP8oaSSD9PxExGvArcC/A6+THe+lDN7zs1Mjz0d//ky5nOyqqlI/fTnme3peK3LyqU1Nj/DJm6SDgP8D/HVE/LpS1RJllR5NlOt4JZ0L/CoilhYWV9iHeo2nv8Y5hOy2yFcj4njgt2S3XMoZKPtdUvqeYhrZLZUjgJHA2RX2YaCfn2ry2M9+GZukz5D9vuPDVfrpy1jqep6cfGoz4B7hI+kAssTzcER8OxW/IenwtP1w4FepfE8fc9STlovLK/WxN04BzpP0KtBBduvty9TvUUt9eZzT3ugBeiLi6bS+gCwZDdbz8yFgVUSsjYjtwLeBDzB4z89OjTwfdf+ZkiZAnAtcFOn+Vx/Gktsjzpx8alPLI4Zyk2bS3Ae8GBFfKthU+AiiS9n90USXpBk2JwOb0y2ARcCZkg5N/7s9k+z+7uvAbySdnPq6hNKPOSrso88i4qaIGB8RLWTHdnFEXET9HrXUl8c57c14/gNYLenPUtEUsidwDMrzQ3a77WRJB6b+do5nUJ6fAo08H+X66BNJU8meDnNeRPyuaIwD8xFnff3ybn/7kM1O+QXZTI7PNHhfTiW7rH0eeDZ9/ors/uv3gZfTn4el+iJ7ed8rwAtAW0Gsy4Hu9PmvBeVtwM9Tmzv4w9MwSvZRx7Gdzh9mux2d/hJ3A98ChqXy4Wm9O20/uqD9Z9I+ryDNOKp0/sr1UYdx/DnQmc7RP5PNjhq05we4GXgp9fkQ2cymQXN+gG+SfV+1nex/6lc08nxU6qOPY+km+95l58+Du+t9zPtyXit9/HgdMzPLnW+7mZlZ7px8zMwsd04+ZmaWOycfMzPLnZOPmZnlzsnHzMxy5+RjZma5+//4PboBTxmDWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df[df['narrow'] == 1]['length'],normed = True,histtype = 'step',label = 'corrupt')\n",
    "plt.hist(df[df['narrow'] == 0]['length'],normed = True,histtype = 'step',label = 'clean')\n",
    "plt.title('Histogram of text length')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that audit documents for corrupt municipalities is much longer than the one for clean munincipalities\n",
    "\n",
    "Now let's see what words appear most frequently in documents for corrupted and clean munincipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "## order the appearing word items by frequency\n",
    "corrupt_narrow = sorted(n.word_counts.items(), key=operator.itemgetter(1),reverse = True)\n",
    "clean = sorted(c.word_counts.items(), key=operator.itemgetter(1),reverse = True)\n",
    "\n",
    "## translate the most frequent words\n",
    "translator = Translator()\n",
    "translated = translator.translate(text = 'medir',src = 'pt',dest = 'en')\n",
    "\n",
    "## method for translating each word\n",
    "def trans(text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(text,src= 'pt',dest='en')\n",
    "    return translated.text\n",
    "\n",
    "## making a list of translated words from the most frequent words for clean munincipalities\n",
    "transclean = {}\n",
    "words = dict(clean[:20]).keys()\n",
    "for word in words:\n",
    "    freq = dict(clean).get(word)\n",
    "    transclean[trans(word)] = freq    \n",
    "    \n",
    "## making a list of translated words from the most frequent words for corrupted munincipalities\n",
    "transcorrupt = {}\n",
    "words = dict(corrupt_narrow[:20]).keys()\n",
    "for word in words:\n",
    "    freq = dict(corrupt_narrow).get(word)\n",
    "    transcorrupt[trans(word)] = freq   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent words in audit reports for corrupted munincipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'municipal': 211702,\n",
       " 'resource': 195291,\n",
       " 'control': 146026,\n",
       " 'public': 130882,\n",
       " 'schedule': 122672,\n",
       " 'unity': 101915,\n",
       " 'federal': 116652,\n",
       " 'secretary': 115879,\n",
       " 'town hall': 109077,\n",
       " 'intern': 108976,\n",
       " 'finding': 106874,\n",
       " 'County': 106509,\n",
       " 'to be': 104541,\n",
       " 'application': 100938,\n",
       " 'fact': 100721,\n",
       " 'general': 95852,\n",
       " 'controllership': 93182,\n",
       " 'greet': 92412,\n",
       " 'service': 89361}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcorrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent words in audit reports for non-corrupt munincipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'municipal': 105198,\n",
       " 'resource': 102663,\n",
       " 'schedule': 81386,\n",
       " 'control': 73101,\n",
       " 'public': 69806,\n",
       " 'County': 67164,\n",
       " 'federal': 61993,\n",
       " 'secretary': 58821,\n",
       " 'inspection': 55333,\n",
       " 'town hall': 54119,\n",
       " 'unity': 44209,\n",
       " 'greet': 53343,\n",
       " 'fact': 51385,\n",
       " 'intern': 51262,\n",
       " 'general': 50189,\n",
       " 'application': 49687,\n",
       " 'controllership': 48760,\n",
       " 'to be': 47201,\n",
       " 'finding': 45340}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm.. the frequently appearing words do not differ much between the corrupt munincipalities and clean munincipalities.\n",
    "Let's sort the words by IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## For the clean municipalities\n",
    "## Calculate the idf\n",
    "ctext = c.texts_to_matrix(df[df['narrow'] == 0]['contents'])\n",
    "idf = np.log(c.document_count/(1+sum(ctext)))\n",
    "## sort the words by IDF\n",
    "newword_clean = {}\n",
    "words= c.word_counts.keys()\n",
    "for word in words:\n",
    "    newword_clean[word]  = c.word_counts.get(word)*idf[c.word_index.get(word)-1]   \n",
    "clean = sorted(newword_clean.items(), key=operator.itemgetter(1),reverse = True)\n",
    "\n",
    "\n",
    "## For the corrupt municipalities\n",
    "## Calculate idf\n",
    "ntext  =  n.texts_to_matrix(df[df['narrow'] == 1]['contents'])\n",
    "idf = np.log(n.document_count/(1+sum(ntext)))\n",
    "## Sort the words by IDF\n",
    "newword = {}\n",
    "words= n.word_counts.keys()\n",
    "for word in words:\n",
    "    newword[word]  = n.word_counts.get(word)*idf[n.word_index.get(word)-1]   \n",
    "corrupt = sorted(newword.items(), key=operator.itemgetter(1),reverse = True)\n",
    "\n",
    "## making a list of translated words from the most frequent words for clean munincipalities\n",
    "transclean = {}\n",
    "words = dict(clean[:30]).keys()\n",
    "for word in words:\n",
    "    idf = dict(clean).get(word)\n",
    "    transclean[trans(word)] = idf   \n",
    "    \n",
    "## making a list of translated words from the most frequent words for corrupted munincipalities\n",
    "transcorrupt = {}\n",
    "words = dict(corrupt[:30]).keys()\n",
    "for word in words:\n",
    "    idf = dict(corrupt).get(word)\n",
    "    transcorrupt[trans(word)] = idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's see the sorted list of words by tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest TFIDF words in reports for corrupted munincipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'municipal': 1449961.0301660683,\n",
       " 'to receive': 25643.243889247526,\n",
       " 'measure': 19287.887741849838,\n",
       " 'basic': 18112.790431703084,\n",
       " 'sample': 17470.258938323845,\n",
       " 'administrative': 17217.612054678517,\n",
       " 'fix': 12070.20868692855,\n",
       " 'last': 11426.632592027092,\n",
       " 'pray': 9632.38860007867,\n",
       " 'pharmaceutical': 9248.624213766905,\n",
       " 'to engineer': 9222.520812774343,\n",
       " 'corrective': 9022.636808824016,\n",
       " 'total': 8492.834318043431,\n",
       " 'consider': 8483.324437518393,\n",
       " 'list': 7645.324093488712,\n",
       " 'totality': 7436.28606968742,\n",
       " 'motivate': 7308.480709658138,\n",
       " 'base': 7142.4384504432965,\n",
       " 'education': 6334.727223002882,\n",
       " 'sound': 6249.240100083505,\n",
       " 'quarantine': 5835.247587366123,\n",
       " 'knowledge': 5768.071287661438,\n",
       " 'bigger': 5614.166092441872,\n",
       " 'OMG': 5603.11496676232,\n",
       " 'to distribute': 5451.858202111344,\n",
       " 'transf': 5450.187464252229,\n",
       " 'third': 5412.783562816628,\n",
       " 'module': 5399.6742426847095,\n",
       " 'economy': 5393.083351039594,\n",
       " 'recreation': 5375.85870782454}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcorrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest TFIDF words in reports for corrupted munincipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'municipal': 694864.1306879089,\n",
       " 'physicist': 13220.7623263255,\n",
       " 'peti': 11532.909903042308,\n",
       " 'documentation': 7873.648800526305,\n",
       " 'young': 6422.302483053103,\n",
       " 'worker': 6147.412631445394,\n",
       " 'December': 6117.224257573882,\n",
       " 'install': 5904.702927912213,\n",
       " 'understand': 5763.59987829596,\n",
       " 'ten': 5440.516106535588,\n",
       " 'source': 5326.55657363016,\n",
       " 'fax': 5255.31337188008,\n",
       " 'go along': 5244.407604405705,\n",
       " 'Antonio': 5192.702177320015,\n",
       " 'Restroom': 4910.4031625014195,\n",
       " 'tell': 4828.77691510397,\n",
       " 'elide': 4814.723547316084,\n",
       " 'settlement': 4607.438444512516,\n",
       " 'establishment': 4380.279739562642,\n",
       " 'laugh': 4293.372951623291,\n",
       " 'to feed': 4291.615943400998,\n",
       " 'job': 4246.372255592074,\n",
       " 'Mount': 4224.135072043043,\n",
       " 'release': 4089.2925733193274,\n",
       " 'to adopt': 4048.091218207125,\n",
       " 'installation': 3895.2425778622646,\n",
       " 'supply': 3884.741931565225,\n",
       " 'to file': 3861.393864261163,\n",
       " 'emef': 3853.7525261200367,\n",
       " 'manifest': 3778.277282996834}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "PETI stands for Programa de Erradicação do Trabalho Infantil (PETI). It is a conditional cash transfer programme with emphasis on child labor\n",
    "\n",
    "We can see that after sorting the words by tfid(term frequency inverse document frequency), there appear to be differences in the words in documents for corrupted munincipaities and clean munincipalities.\n",
    "\n",
    "However, it is hard to pinpoint which words could be distinguishing corrupt munincipalities from clean munincipalities, sinece we have not considered negations. (e.g. 'distribute' could actually appear as 'do not distribute' in the orininal text). This may be better to use 2 bag of words for the classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Finally, let's train predictive models.\n",
    "We will first try with a basic model where each document is classified simply based on the presence of the word \"corrupt\" and its synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction accuracy for base classifier: 0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## helper function to translate from English to Portuguese\n",
    "def trans(text):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(text,src= 'en',dest='pt')\n",
    "    return translated.text\n",
    "## helper function to check if the document contains the word corrupt\n",
    "def containsText(text):\n",
    "    for syn in synonymPT:\n",
    "        if syn in text:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Obtains the list of sysnonyms of \"corrupt\" in English\n",
    "syns = wordnet.synsets(\"corrupt\")\n",
    "synonyms = []\n",
    "for syn in syns:\n",
    "\t\tfor l in syn.lemmas():\n",
    "\t\t\tsynonyms.append(l.name())\n",
    "\n",
    "# Translate each word in Portuguese\n",
    "synonymsPT = []\n",
    "for word in synonyms:\n",
    "    tr = trans(word)\n",
    "    synonymsPT.append(tr)\n",
    "synonymPT = set(synonymsPT)\n",
    "\n",
    "## create a new variable contains (1 if there exists a word \"corrupt\" or its sysnomyms, 0 otherwise)\n",
    "df['contains'] = df['contents'].apply(containsText)\n",
    "\n",
    "## divide them into training and test data\n",
    "y = np.array(df['narrow'].values).reshape(-1,1)\n",
    "X = np.array(df['contains'].values).reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "lr_base = LogisticRegression(random_state = 0, solver = 'lbfgs').fit(X_train,Y_train\n",
    "score = lr_base.score(X_test,Y_test)\n",
    "print(\"Test prediction accuracy for base classifier: %.3f\"% score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now classify the document based on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the preprocessed data\n",
    "df = pd.read_csv('data/corruption/processed.csv',encoding = 'utf-8')\n",
    "\n",
    "\n",
    "## divide them into training and test data\n",
    "y = df['narrow'].values\n",
    "X = df['contents'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 29.6min finished\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Default Vectorizer \n",
    "tfidf = TfidfVectorizer(use_idf = True, max_df = 0.95, min_df = 0.05)\n",
    "\n",
    "## Logistic Regression\n",
    "LRpipe = Pipeline([('vect', tfidf),\n",
    "                    ('clf',LogisticRegression(random_state = 0))])\n",
    "param_grid = [{'vect__ngram_range': [[1,2],[2,3]],\n",
    "               'clf__penalty':['l1', 'l2'],\n",
    "               'clf__C':[1.0, 10.0,100.0]}]\n",
    "gs_lr_tfidf = GridSearchCV(LRpipe, param_grid,scoring = 'accuracy', cv = 3, verbose = 1, n_jobs = -1)\n",
    "gs_lr_tfidf.fit(X_train,Y_train)\n",
    "lrclf = gs_lr_tfidf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For SVC with linear kernel\n",
    "SVMpipe = Pipeline([('vect', tfidf),('SVM', svm.SVC(kernel = 'linear',probability = True))])\n",
    "\n",
    "param_grid = {\n",
    "    'vect__ngram_range':[[1,2],[2,3]],\n",
    "    'vect__min_df':[0.05],\n",
    "    'vect__max_df':[0.95],\n",
    "    'SVM__C': [0.01, 0.1, 1,10]\n",
    "}\n",
    "                    \n",
    "SVM = GridSearchCV(SVMpipe,param_grid,cv=3,n_jobs = 1)\n",
    "SVM.fit(X_train,Y_train)\n",
    "svcclf = SVM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Naive Bayes\n",
    "NBpipe = Pipeline([('vect', tfidf),\n",
    "                   ('clf',MultinomialNB())])\n",
    "param_grid = {\n",
    "    'vect__ngram_range':[[1,2],[2,3]],\n",
    "    'vect__min_df':[0.05],\n",
    "    'vect__max_df':[0.95],\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "NB = GridSearchCV(NBpipe,param_grid,cv=3,n_jobs = 1)\n",
    "NB.fit(X_train,Y_train)\n",
    "NBclf = NB.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##XGBOOST\n",
    "import xgboost as xgb\n",
    "XGBpipe = Pipeline([('vect', tfidf),\n",
    "                   ('clf',xgb.XGBClassifier(scale_pos_weight = 1, learning_rate = 0.1,reg_alpha = 0.3, max_depth = 6,n_estimators = 100,subsample = 1))])\n",
    "param_grid = {\n",
    "    'vect__ngram_range':[[1,2]],\n",
    "    'vect__min_df':[0.05],\n",
    "    'vect__max_df':[0.95],\n",
    "    'clf__colsample_bytree': [0.3, 0.4, 0.5],\n",
    "    'clf__gamma':[0,1,3],\n",
    "    'clf__reg_alpha':[0,0.1,0.5]\n",
    "}\n",
    "\n",
    "XGB = GridSearchCV(XGBpipe, param_grid,cv = 3, n_jobs = -1)\n",
    "XGB.fit(X_train,Y_train)\n",
    "XGBclf = XGB.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the classifiers for the future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## saving the fitted models for future use\n",
    "filenames = ['lr_model.sav', 'svm_model.sav','nb_model.sav','xgb_model.sav']\n",
    "models = [lrclf, svcclf, NBclf, XGBclf]\n",
    "for i in range(len(filenames)):\n",
    "    pickle.dump(models[i], open(filenames[i], 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "models = {}\n",
    "filenames = ['lr_model.sav', 'svm_model.sav','nb_model.sav','xgb_model.sav']\n",
    "for file in filenames:\n",
    "    model = pickle.load(open(file, 'rb'))\n",
    "    models[file[:-4]] = model\n",
    "clflist = []\n",
    "for model in models:\n",
    "    clflist.append((model,models[model]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing text scores for each classifiers\n",
      "lr_model : 0.673\n",
      "svm_model : 0.673\n",
      "nb_model : 0.649\n",
      "xgb_model : 0.710\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "print(\"Printing text scores for each classifiers\")\n",
    "for model in models:\n",
    "    score = models.get(model).score(X_test,Y_test)\n",
    "    print(\"%s : %.3f\" %(model, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB has the best test accuracy score. They all perform better than our base model with the accuracy of 56 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to the input variable\n",
    "Let's now add a features such as the number of images per document and the length of the document\n",
    "\n",
    "We will implement this by using Pipeline with customized transformers. We will approach this in 2 different ways.\n",
    "1. merge the vectorized text with feature variables(# of images and pages) as input\n",
    "2. implement separate classifiers for text data and feature data, then we will take majority vote(soft).\n",
    "\n",
    "We will first implement custome transformer classes to be used in our pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/corruption/processed.csv',encoding = 'utf-8')\n",
    "\n",
    "y = df['narrow'].values\n",
    "X = df\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "##helper function to return the word counts per document\n",
    "def textlength(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Custom transformer to create new variable of length of each document\n",
    "class NumericalTransformer(BaseEstimator, TransformerMixin):\n",
    " \n",
    "    #class Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X, y = None):\n",
    "        con = X.loc[:,'contents']\n",
    "        page = np.array(X.loc[:,'page']).reshape(-1,1)\n",
    "        img = np.array(X.loc[:,'num_img']).reshape(-1,1)\n",
    "        l = np.array(con.apply(textlength)).reshape(-1,1)\n",
    "        l = np.hstack([l,page,img])\n",
    "        return l\n",
    "# Custon transformer to extract text \n",
    "class DocumentSelector(BaseEstimator, TransformerMixin):\n",
    " \n",
    "    #class Constructor\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X, y = None):\n",
    "        text = X['contents']\n",
    "        return text\n",
    "    \n",
    "# Custom transformer to vectorize the text data into sparse matrix\n",
    "class TfidfTransformer():\n",
    "    def __init__(self,max_df = 0.9):\n",
    "        self.max_df = max_df\n",
    "        \n",
    "    def fit(self,X,Y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,Y = None):\n",
    "        X = X.loc[:,'contents']\n",
    "        tfidf = TfidfVectorizer(ngram_range=[1,2],max_df = self.max_df, min_df = 0.1, use_idf = True, lowercase = False)\n",
    "        X = tfidf.fit_transform(X)\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features: Trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will process and train our predictive model using pipeline as shown below\n",
    "<img src=\"pipeline1.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the steps in the pipelines   \n",
    "numerical_pipeline = Pipeline( steps = [ \n",
    "                                  ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  ( 'std_scaler', StandardScaler() ) ] )\n",
    "text_pipeline = Pipeline(steps = [\n",
    "                                ('documentExtract', DocumentSelector()),\n",
    "                                ('tfid', TfidfVectorizer(ngram_range=[1,2],max_df = 0.95, min_df = 0.05, use_idf = True, lowercase = False)),\n",
    "])\n",
    "input_pipeline = FeatureUnion(transformer_list = [('text_pipeline', text_pipeline),\n",
    "                                                 ('num_pipeline', numerical_pipeline )])\n",
    "full_pipeline = Pipeline(steps = [('input',input_pipeline),\n",
    "                                                 ('model', xgb.XGBClassifier(scale_pos_weight = 1, learning_rate = 0.1,reg_alpha = 0.3, max_depth = 6,n_estimators = 100,subsample = 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "## setting gridsearch paratemers\n",
    "param_grid = {\n",
    "    'model__colsample_bytree': [0.3, 0.4, 0.5],\n",
    "    'model__gamma':[0,1,3],\n",
    "    'model__reg_alpha':[0,0.1,0.5]\n",
    "}\n",
    "gs_full_pipeline = GridSearchCV(full_pipeline, param_grid,scoring = 'accuracy', cv = 3, verbose = 1, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed: 92.9min finished\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#training gridsearch\n",
    "gs_full_pipeline.fit(X_train,Y_train)\n",
    "xgb2 = gs_full_pipeline.best_estimator_\n",
    "## saving the best estimator\n",
    "pickle.dump(xgb2, open(\"xgb2_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6996402877697842"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, seems like that adding features did not improve the performance. Let's try different way of addting features.\n",
    "This time, we will have a separate classifier for the features and we will cooperate this classifier with text classifier by majority vote classifier\n",
    "### Adding features: Trial 2\n",
    "We will construct pipeline in the following way\n",
    "<img src=\"pipeline2.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the steps in the numerical pipeline     \n",
    "pipe1 = Pipeline( steps = [ \n",
    "                                  ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  ( 'std_scaler', StandardScaler() ),\n",
    "                                  ('model', LogisticRegression(solver = 'lbfgs'))\n",
    "] )\n",
    "\n",
    "# Definint the steps in the text pipeline\n",
    "pipe2 = Pipeline(steps = [\n",
    "                                ('documentExtract', DocumentSelector()),\n",
    "                                ('tfid', TfidfVectorizer(ngram_range=[1,2],max_df = 0.95, min_df = 0.05, use_idf = True, lowercase = False)),\n",
    "                                ('model', xgb.XGBClassifier(scale_pos_weight = 1, learning_rate = 0.1,reg_alpha = 0.3, max_depth = 6,n_estimators = 100,subsample = 1))\n",
    "                                \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "## Building classifier for majority vote\n",
    "eclf = EnsembleVoteClassifier(clfs=[pipe1,pipe2], \n",
    "                              voting='soft')\n",
    "\n",
    "param_grid = [{'pipeline-1__model__C':[1.0, 10.0],\n",
    "               'pipeline-2__model__colsample_bytree': [0.3, 0.4, 0.5],\n",
    "               'pipeline-2__model__gamma':[0,1,3],\n",
    "               'pipeline-2__model__reg_alpha':[0,0.1,0.5]\n",
    "                    }]\n",
    "gs_full_pipeline = GridSearchCV(eclf, param_grid,scoring = 'accuracy', cv = 3, verbose = 1, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 186.3min finished\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=EnsembleVoteClassifier(clfs=[Pipeline(memory=None,\n",
       "     steps=[('num_transformer', NumericalTransformer()), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_s...      subsample=1, verbosity=1))])],\n",
       "            refit=True, verbose=0, voting='soft', weights=None),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'pipeline-1__model__C': [1.0, 10.0], 'pipeline-2__model__colsample_bytree': [0.3, 0.4, 0.5], 'pipeline-2__model__gamma': [0, 1, 3], 'pipeline-2__model__reg_alpha': [0, 0.1, 0.5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_full_pipeline.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best estimator\n",
    "import pickle\n",
    "ensemble_clf = gs_full_pipeline.best_estimator_\n",
    "pickle.dump(ensemble_clf, open(\"ensemble_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032374100719424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, adding features did not improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
